{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda899c7",
   "metadata": {},
   "source": [
    "# 09 — CNN (TextCNN)\n",
    "\n",
    "A **Convolutional Neural Network** for text classification using Word2Vec embeddings.\n",
    "\n",
    "Unlike the FFN (which averaged word vectors), this model operates on **sequences** of word vectors,\n",
    "using Conv1D filters at multiple window sizes (3, 4, 5) to capture n-gram patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b671a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:49.420390Z",
     "iopub.status.busy": "2026-02-16T03:20:49.419901Z",
     "iopub.status.idle": "2026-02-16T03:20:53.342348Z",
     "shell.execute_reply": "2026-02-16T03:20:53.341276Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffd6bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:53.346433Z",
     "iopub.status.busy": "2026-02-16T03:20:53.345827Z",
     "iopub.status.idle": "2026-02-16T03:20:53.392705Z",
     "shell.execute_reply": "2026-02-16T03:20:53.391758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.12.12\n",
      "IPython version      : 9.10.0\n",
      "\n",
      "numpy  : 1.26.4\n",
      "pandas : 3.0.0\n",
      "torch  : 2.2.2\n",
      "gensim : 4.4.0\n",
      "sklearn: 1.8.0\n",
      "\n",
      "Compiler    : Clang 17.0.0 (clang-1700.6.3.2)\n",
      "OS          : Darwin\n",
      "Release     : 25.2.0\n",
      "Machine     : x86_64\n",
      "Processor   : i386\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,pandas,torch,gensim,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d6e60",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We need **sequences** of word indices (not averaged vectors) so that Conv1D can detect local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e00ee60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:53.395821Z",
     "iopub.status.busy": "2026-02-16T03:20:53.395413Z",
     "iopub.status.idle": "2026-02-16T03:20:53.401596Z",
     "shell.execute_reply": "2026-02-16T03:20:53.401028Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 50  # Pad/truncate all documents to 50 tokens\n",
    "EMBED_DIM = 100\n",
    "LABEL_MAP = {'NEGATIVE': 0, 'POSITIVE': 1}\n",
    "\n",
    "def build_embedding_matrix(w2v_model):\n",
    "    \"\"\"Create an embedding matrix from Word2Vec. Index 0 is reserved for padding.\"\"\"\n",
    "    vocab = w2v_model.wv.key_to_index\n",
    "    matrix = np.zeros((len(vocab) + 1, EMBED_DIM))  # +1 for padding at index 0\n",
    "    word2idx = {'<PAD>': 0}\n",
    "    for word, idx in vocab.items():\n",
    "        word2idx[word] = idx + 1  # Shift by 1 (0 = padding)\n",
    "        matrix[idx + 1] = w2v_model.wv[word]\n",
    "    return matrix, word2idx\n",
    "\n",
    "def texts_to_sequences(texts, word2idx, max_len):\n",
    "    \"\"\"Convert texts to padded integer sequences.\"\"\"\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        tokens = str(text).split()\n",
    "        seq = [word2idx.get(w, 0) for w in tokens[:max_len]]\n",
    "        # Pad\n",
    "        seq += [0] * (max_len - len(seq))\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990da4a4",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "TextCNN: Embedding → Multiple Conv1D filters (sizes 3, 4, 5) → MaxPool → Concatenate → Dense → Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953a1dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:53.403946Z",
     "iopub.status.busy": "2026-02-16T03:20:53.403735Z",
     "iopub.status.idle": "2026-02-16T03:20:53.410225Z",
     "shell.execute_reply": "2026-02-16T03:20:53.409497Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_filters=100, filter_sizes=(3, 4, 5), dropout=0.3):\n",
    "        super().__init__()\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Pre-trained embedding layer (frozen)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "        \n",
    "        # Conv1D filters at different window sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embed_dim, num_filters, kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len) → (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(x)\n",
    "        # Conv1d expects (batch, channels, seq_len)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply each conv filter + max-over-time pooling\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            c = F.relu(conv(x))           # (batch, num_filters, new_len)\n",
    "            c = F.max_pool1d(c, c.size(2)) # (batch, num_filters, 1)\n",
    "            c = c.squeeze(2)               # (batch, num_filters)\n",
    "            conv_outs.append(c)\n",
    "        \n",
    "        # Concatenate all filter outputs\n",
    "        out = torch.cat(conv_outs, dim=1)  # (batch, num_filters * len(filter_sizes))\n",
    "        out = self.dropout(out)\n",
    "        out = torch.sigmoid(self.fc(out)).squeeze(1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b411ae",
   "metadata": {},
   "source": [
    "## 3. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4de5324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:53.412835Z",
     "iopub.status.busy": "2026-02-16T03:20:53.412549Z",
     "iopub.status.idle": "2026-02-16T03:20:53.420373Z",
     "shell.execute_reply": "2026-02-16T03:20:53.419804Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_cnn(variation_name, data_dir, w2v_path, output_dir, epochs=20, lr=1e-3, batch_size=32):\n",
    "    print(f\"\\n{'='*20} TextCNN: {variation_name} {'='*20}\")\n",
    "    \n",
    "    # Load Word2Vec\n",
    "    w2v = Word2Vec.load(w2v_path)\n",
    "    embed_matrix, word2idx = build_embedding_matrix(w2v)\n",
    "    print(f\"Embedding matrix: {embed_matrix.shape}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(f'{data_dir}/train.csv').fillna('')\n",
    "    test_df  = pd.read_csv(f'{data_dir}/test.csv').fillna('')\n",
    "    \n",
    "    X_train = texts_to_sequences(train_df['text_clean'], word2idx, MAX_LEN)\n",
    "    X_test  = texts_to_sequences(test_df['text_clean'], word2idx, MAX_LEN)\n",
    "    y_train = train_df['label'].map(LABEL_MAP).values.astype(np.float32)\n",
    "    y_test  = test_df['label'].map(LABEL_MAP).values.astype(np.float32)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_ds = TensorDataset(torch.LongTensor(X_train), torch.FloatTensor(y_train))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Model\n",
    "    model = TextCNN(embed_matrix)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=lr\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs} — Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(torch.LongTensor(X_test))\n",
    "        y_pred = (preds >= 0.5).int().numpy()\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nTextCNN ({variation_name}) Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_test.astype(int), y_pred))\n",
    "    \n",
    "    # Save\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), f'{output_dir}/model.pt')\n",
    "    print(f\"Model saved to {output_dir}/model.pt\")\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0ba88",
   "metadata": {},
   "source": [
    "## 4. Run All Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06e0844",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:20:53.422878Z",
     "iopub.status.busy": "2026-02-16T03:20:53.422653Z",
     "iopub.status.idle": "2026-02-16T03:21:45.712998Z",
     "shell.execute_reply": "2026-02-16T03:21:45.711579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== TextCNN: Standard ====================\n",
      "Embedding matrix: (2248, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/20 — Loss: 0.3536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/20 — Loss: 0.2006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/20 — Loss: 0.1121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20 — Loss: 0.0729\n",
      "\n",
      "TextCNN (Standard) Accuracy: 0.8200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.83       225\n",
      "           1       0.84      0.79      0.81       225\n",
      "\n",
      "    accuracy                           0.82       450\n",
      "   macro avg       0.82      0.82      0.82       450\n",
      "weighted avg       0.82      0.82      0.82       450\n",
      "\n",
      "Model saved to ../models/cnn/standard/model.pt\n",
      "\n",
      "==================== TextCNN: Irony ====================\n",
      "Embedding matrix: (2239, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/20 — Loss: 0.3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/20 — Loss: 0.1965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/20 — Loss: 0.1125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20 — Loss: 0.0649\n",
      "\n",
      "TextCNN (Irony) Accuracy: 0.8067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       225\n",
      "           1       0.84      0.75      0.80       225\n",
      "\n",
      "    accuracy                           0.81       450\n",
      "   macro avg       0.81      0.81      0.81       450\n",
      "weighted avg       0.81      0.81      0.81       450\n",
      "\n",
      "Model saved to ../models/cnn/irony/model.pt\n",
      "\n",
      "==================== TextCNN: Obfuscated ====================\n",
      "Embedding matrix: (2237, 100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 5/20 — Loss: 0.3421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 10/20 — Loss: 0.1890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 15/20 — Loss: 0.1167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch 20/20 — Loss: 0.0639\n",
      "\n",
      "TextCNN (Obfuscated) Accuracy: 0.8089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.85      0.82       225\n",
      "           1       0.84      0.77      0.80       225\n",
      "\n",
      "    accuracy                           0.81       450\n",
      "   macro avg       0.81      0.81      0.81       450\n",
      "weighted avg       0.81      0.81      0.81       450\n",
      "\n",
      "Model saved to ../models/cnn/obfuscated/model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "acc_standard = train_cnn(\n",
    "    \"Standard\",\n",
    "    \"../data/processed/standard\",\n",
    "    \"../models/word2vec/standard/word2vec.model\",\n",
    "    \"../models/cnn/standard\"\n",
    ")\n",
    "\n",
    "acc_irony = train_cnn(\n",
    "    \"Irony\",\n",
    "    \"../data/processed/irony\",\n",
    "    \"../models/word2vec/irony/word2vec.model\",\n",
    "    \"../models/cnn/irony\"\n",
    ")\n",
    "\n",
    "acc_obfuscated = train_cnn(\n",
    "    \"Obfuscated\",\n",
    "    \"../data/processed/obfuscated\",\n",
    "    \"../models/word2vec/obfuscated/word2vec.model\",\n",
    "    \"../models/cnn/obfuscated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb152a98",
   "metadata": {},
   "source": [
    "## 5. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72df0dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T03:21:45.716639Z",
     "iopub.status.busy": "2026-02-16T03:21:45.716257Z",
     "iopub.status.idle": "2026-02-16T03:21:45.722674Z",
     "shell.execute_reply": "2026-02-16T03:21:45.721668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Comparison ===\n",
      "Standard: 0.8200\n",
      "Irony:    0.8067\n",
      "Obfuscated: 0.8089\n",
      "Impact of Irony features: -0.0133\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Comparison ===\")\n",
    "print(f\"Standard: {acc_standard:.4f}\")\n",
    "print(f\"Irony:    {acc_irony:.4f}\")\n",
    "print(f\"Obfuscated: {acc_obfuscated:.4f}\")\n",
    "diff = acc_irony - acc_standard\n",
    "print(f\"Impact of Irony features: {diff:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
